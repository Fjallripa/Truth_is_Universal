{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# repository imports\n",
    "from utils import DataManager, dataset_sizes, collect_training_data, compute_statistics, compute_average_accuracies\n",
    "from probes import CCSProbe, TTPD, LRProbe, MMProbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "model_family = 'Llama3' # options are 'Llama3', 'Llama2', 'Gemma', 'Gemma2' or 'Mistral'\n",
    "model_size = '8B'\n",
    "model_type = 'chat' # options are 'chat' or 'base'\n",
    "layer = 12 # layer from which to extract activations\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu' # gpu speeds up CCS training a fair bit but is not required\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets used for training\n",
    "train_sets = [\"cities\", \"neg_cities\", \"sp_en_trans\", \"neg_sp_en_trans\", \"inventors\", \"neg_inventors\", \"animal_class\",\n",
    "                  \"neg_animal_class\", \"element_symb\", \"neg_element_symb\", \"facts\", \"neg_facts\"]  # len = 12\n",
    "# get size of each training dataset to include an equal number of statements from each topic in training data\n",
    "train_set_sizes = dataset_sizes(train_sets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseen topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# compare TTPD, LR and CCS on topic-specific datasets\n",
    "probe_types = [TTPD, LRProbe, CCSProbe, MMProbe]\n",
    "results = {\n",
    "    TTPD: defaultdict(list), \n",
    "    LRProbe: defaultdict(list), \n",
    "    CCSProbe: defaultdict(list), \n",
    "    MMProbe: defaultdict(list)}\n",
    "num_iter = 20\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter * len(train_sets)   # number_of_probes * number_of_iterations * number_of_datasets\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            indices = np.arange(0, 12, 2)   # indices of the datasets to be used for training\n",
    "            for i in indices:\n",
    "                cv_train_sets = np.delete(np.array(train_sets), [i, i+1], axis=0)\n",
    "                # load training data\n",
    "                acts_centered, acts, labels, polarities = collect_training_data(cv_train_sets, train_set_sizes, model_family, model_size, model_type, layer)\n",
    "                \n",
    "                # train classifier\n",
    "                if probe_type == TTPD:\n",
    "                    probe = TTPD.from_data(acts_centered, acts, labels, polarities)\n",
    "                if probe_type == LRProbe:\n",
    "                    probe = LRProbe.from_data(acts, labels)\n",
    "                if probe_type == CCSProbe:\n",
    "                    acts_affirm = acts[polarities == 1.0]\n",
    "                    acts_neg = acts[polarities == -1.0]\n",
    "                    labels_affirm = labels[polarities == 1.0]\n",
    "                    mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                    mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                    acts_affirm = acts_affirm - mean_affirm\n",
    "                    acts_neg = acts_neg - mean_neg\n",
    "                    probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "                if probe_type == MMProbe:\n",
    "                    probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "                # evaluate classification accuracy on held out datasets\n",
    "                dm = DataManager()\n",
    "                for j in range(0,2):\n",
    "                    dm.add_dataset(train_sets[i+j], model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                    acts, labels = dm.data[train_sets[i+j]]\n",
    "                    \n",
    "                    # classifier specific predictions\n",
    "                    if probe_type == CCSProbe:\n",
    "                        if j == 0:\n",
    "                            acts = acts - mean_affirm\n",
    "                        if j == 1:\n",
    "                            acts = acts - mean_neg\n",
    "                    predictions = probe.pred(acts)\n",
    "                    results[probe_type][train_sets[i+j]].append((predictions == labels).float().mean().item())\n",
    "                    pbar.update(1)\n",
    "\n",
    "stat_results = compute_statistics(results)\n",
    "\n",
    "# Compute mean accuracies and standard deviations for each probe type\n",
    "probe_accuracies = compute_average_accuracies(results, num_iter)\n",
    "\n",
    "for probe_type, stats in probe_accuracies.items():\n",
    "    print(f\"{probe_type}:\")\n",
    "    print(f\"  Mean Accuracy: {stats['mean']*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation of the mean accuracy: {stats['std_dev']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(figsize=(7, 6), ncols=4)\n",
    "titles = [\"TTPD\", \"LR\", \"CCS\", \"MM\"]\n",
    "\n",
    "for t, (ax, key) in enumerate(zip((ax1, ax2, ax3, ax4), (TTPD, LRProbe, CCSProbe, MMProbe))):\n",
    "    grid = [[stat_results[key]['mean'][dataset]] for dataset in train_sets]\n",
    "    grid_std = [[stat_results[key]['std'][dataset]] for dataset in train_sets]\n",
    "    im = ax.imshow(grid, vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "    \n",
    "    for i, row in enumerate(grid):\n",
    "        for j, val in enumerate(row):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d} $\\pm$ {round(grid_std[i][j] * 100):2d}', ha='center', va='center', fontsize=13)\n",
    "    \n",
    "    ax.set_yticks(range(len(train_sets)))\n",
    "    ax.set_xticks([])\n",
    "    \n",
    "    ax.set_title(titles[t], fontsize=14)\n",
    "\n",
    "ax1.set_yticklabels(train_sets, fontsize=13)\n",
    "ax2.set_yticklabels([])\n",
    "ax3.set_yticklabels([])\n",
    "ax4.set_yticklabels([])\n",
    "\n",
    "cbar = fig.colorbar(im, ax=[ax1, ax2, ax3, ax4])\n",
    "cbar.ax.tick_params(labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Classification accuracies\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalisation to logical conjunctions and disjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# compare TTPD, LR, CCS and MM on logical conjunctions and disjunctions\n",
    "val_sets = [\"cities_conj\", \"cities_disj\", \"sp_en_trans_conj\",\"sp_en_trans_disj\",\n",
    "             \"inventors_conj\", \"inventors_disj\", \"animal_class_conj\", \"animal_class_disj\",\n",
    "               \"element_symb_conj\", \"element_symb_disj\", \"facts_conj\", \"facts_disj\",\n",
    "            \"common_claim_true_false\", \"counterfact_true_false\"]\n",
    "\n",
    "\n",
    "probe_types = [TTPD, LRProbe, CCSProbe, MMProbe]\n",
    "results = {TTPD: defaultdict(list), LRProbe: defaultdict(list), CCSProbe: defaultdict(list), MMProbe: defaultdict(list)}\n",
    "num_iter = 20\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, model_size,\n",
    "                                                                             model_type, layer)\n",
    "            if probe_type == TTPD:\n",
    "                probe = TTPD.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on validation datasets\n",
    "            dm = DataManager()\n",
    "            for val_set in val_sets:\n",
    "                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                acts, labels = dm.data[val_set]\n",
    "                \n",
    "                # classifier specific predictions\n",
    "                if probe_type == CCSProbe:\n",
    "                    acts = acts - (mean_affirm + mean_neg)/2\n",
    "                predictions = probe.pred(acts)\n",
    "                results[probe_type][val_set].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)\n",
    "\n",
    "stat_results = compute_statistics(results)\n",
    "\n",
    "# Compute mean accuracies and standard deviations for each probe type\n",
    "probe_accuracies = compute_average_accuracies(results, num_iter)\n",
    "\n",
    "for probe_type, stats in probe_accuracies.items():\n",
    "    print(f\"{probe_type}:\")\n",
    "    print(f\"  Mean Accuracy: {stats['mean']*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation of the mean accuracy: {stats['std_dev']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(figsize=(7, 6), ncols=4)\n",
    "titles = [\"TTPD\", \"LR\", \"CCS\", \"MM\"]\n",
    "\n",
    "for t, (ax, key) in enumerate(zip((ax1, ax2, ax3, ax4), (TTPD, LRProbe, CCSProbe, MMProbe))):\n",
    "    grid = [[stat_results[key]['mean'][dataset]] for dataset in val_sets]\n",
    "    grid_std = [[stat_results[key]['std'][dataset]] for dataset in val_sets]\n",
    "    im = ax.imshow(grid, vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "    \n",
    "    for i, row in enumerate(grid):\n",
    "        for j, val in enumerate(row):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d} $\\pm$ {round(grid_std[i][j] * 100):2d}', ha='center', va='center', fontsize=13)\n",
    "    \n",
    "    ax.set_yticks(range(len(val_sets)))\n",
    "    ax.set_xticks([])\n",
    "    \n",
    "    ax.set_title(titles[t], fontsize=14)\n",
    "\n",
    "ax1.set_yticklabels(val_sets, fontsize=13)\n",
    "ax2.set_yticklabels([])\n",
    "ax3.set_yticklabels([])\n",
    "ax4.set_yticklabels([])\n",
    "\n",
    "cbar = fig.colorbar(im, ax=[ax1, ax2, ax3, ax4])\n",
    "cbar.ax.tick_params(labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Classification accuracies\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalisation to German statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# compare TTPD, LR, CCS and MM on statements translated to german\n",
    "val_sets = [\"cities_de\", \"neg_cities_de\", \"sp_en_trans_de\", \"neg_sp_en_trans_de\", \"inventors_de\", \"neg_inventors_de\", \"animal_class_de\",\n",
    "                  \"neg_animal_class_de\", \"element_symb_de\", \"neg_element_symb_de\", \"facts_de\", \"neg_facts_de\"]\n",
    "\n",
    "\n",
    "probe_types = [TTPD, LRProbe, CCSProbe, MMProbe]\n",
    "results = {TTPD: defaultdict(list), LRProbe: defaultdict(list), CCSProbe: defaultdict(list), MMProbe: defaultdict(list)}\n",
    "num_iter = 20\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, model_size,\n",
    "                                                                                           model_type, layer)\n",
    "            if probe_type == TTPD:\n",
    "                probe = TTPD.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on validation datasets\n",
    "            dm = DataManager()\n",
    "            for val_set in val_sets:\n",
    "                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                acts, labels = dm.data[val_set]\n",
    "                \n",
    "                # classifier specific predictions\n",
    "                if probe_type == CCSProbe:\n",
    "                    acts = acts - (mean_affirm + mean_neg)/2\n",
    "                predictions = probe.pred(acts)\n",
    "                \n",
    "                results[probe_type][val_set].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)\n",
    "\n",
    "stat_results = compute_statistics(results)\n",
    "\n",
    "# Compute mean accuracies and standard deviations for each probe type\n",
    "probe_accuracies = compute_average_accuracies(results, num_iter)\n",
    "\n",
    "for probe_type, stats in probe_accuracies.items():\n",
    "    print(f\"{probe_type}:\")\n",
    "    print(f\"  Mean Accuracy: {stats['mean']*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation of the mean accuracy: {stats['std_dev']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(figsize=(7, 6), ncols=4)\n",
    "titles = [\"TTPD\", \"LR\", \"CCS\", \"MM\"]\n",
    "\n",
    "for t, (ax, key) in enumerate(zip((ax1, ax2, ax3, ax4), (TTPD, LRProbe, CCSProbe, MMProbe))):\n",
    "    grid = [[stat_results[key]['mean'][dataset]] for dataset in val_sets]\n",
    "    grid_std = [[stat_results[key]['std'][dataset]] for dataset in val_sets]\n",
    "    im = ax.imshow(grid, vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "    \n",
    "    for i, row in enumerate(grid):\n",
    "        for j, val in enumerate(row):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d} $\\pm$ {round(grid_std[i][j] * 100):2d}', ha='center', va='center', fontsize=13)\n",
    "    \n",
    "    ax.set_yticks(range(len(val_sets)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(titles[t], fontsize=14)\n",
    "\n",
    "ax1.set_yticklabels(val_sets, fontsize=13)\n",
    "ax2.set_yticklabels([])\n",
    "ax3.set_yticklabels([])\n",
    "ax4.set_yticklabels([])\n",
    "\n",
    "cbar = fig.colorbar(im, ax=[ax1, ax2, ax3, ax4])\n",
    "cbar.ax.tick_params(labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Classification accuracies\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying generalisation to Conjunctions, Disjunctions and German statements in one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Define the validation sets and the probe types\n",
    "val_sets = [\"cities_conj\", \"cities_disj\", \"sp_en_trans_conj\",\"sp_en_trans_disj\",\n",
    "             \"inventors_conj\", \"inventors_disj\", \"animal_class_conj\", \"animal_class_disj\",\n",
    "               \"element_symb_conj\", \"element_symb_disj\", \"facts_conj\", \"facts_disj\", \"cities_de\", \"neg_cities_de\", \"sp_en_trans_de\", \"neg_sp_en_trans_de\", \"inventors_de\", \"neg_inventors_de\", \"animal_class_de\",\n",
    "                  \"neg_animal_class_de\", \"element_symb_de\", \"neg_element_symb_de\", \"facts_de\", \"neg_facts_de\",\n",
    "            \"common_claim_true_false\", \"counterfact_true_false\"]\n",
    "\n",
    "probe_types = [TTPD, LRProbe, CCSProbe, MMProbe]\n",
    "results = {TTPD: defaultdict(list), LRProbe: defaultdict(list), CCSProbe: defaultdict(list), MMProbe: defaultdict(list)}\n",
    "num_iter = 20\n",
    "\n",
    "# Training and evaluating classifiers\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar:\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family, model_size,\n",
    "                                                                                           model_type, layer)\n",
    "            if probe_type == TTPD:\n",
    "                probe = TTPD.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on validation datasets\n",
    "            dm = DataManager()\n",
    "            for val_set in val_sets:\n",
    "                dm.add_dataset(val_set, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "                acts, labels = dm.data[val_set]\n",
    "                \n",
    "                # classifier specific predictions\n",
    "                if probe_type == CCSProbe:\n",
    "                    acts = acts - (mean_affirm + mean_neg)/2\n",
    "                predictions = probe.pred(acts)\n",
    "                results[probe_type][val_set].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the groups\n",
    "groups = {\n",
    "    'Conjunctions': [dataset for dataset in val_sets if dataset.endswith('_conj')],\n",
    "    'Disjunctions': [dataset for dataset in val_sets if dataset.endswith('_disj')],\n",
    "    'Affirmative German': [dataset for dataset in val_sets if dataset.endswith('_de') and not dataset.startswith('neg_')],\n",
    "    'Negated German': [dataset for dataset in val_sets if dataset.startswith('neg_') and dataset.endswith('_de')],\n",
    "    'common_claim_true_false': ['common_claim_true_false'],\n",
    "    'counterfact_true_false': ['counterfact_true_false']\n",
    "}\n",
    "\n",
    "# Initialize group results\n",
    "group_results = {probe_type: {group_name: [] for group_name in groups} for probe_type in probe_types}\n",
    "\n",
    "# Process results to compute mean accuracies per group per classifier\n",
    "for probe_type in probe_types:\n",
    "    for n in range(num_iter):\n",
    "        for group_name, group_datasets in groups.items():\n",
    "            accuracies = []\n",
    "            for dataset in group_datasets:\n",
    "                accuracy = results[probe_type][dataset][n]\n",
    "                accuracies.append(accuracy)\n",
    "            mean_accuracy = sum(accuracies) / len(accuracies)\n",
    "            group_results[probe_type][group_name].append(mean_accuracy)\n",
    "\n",
    "# Compute statistics\n",
    "stat_group_results = {probe_type: {'mean': {}, 'std': {}} for probe_type in probe_types}\n",
    "\n",
    "for probe_type in probe_types:\n",
    "    for group_name in groups:\n",
    "        accuracies = group_results[probe_type][group_name]\n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "        std_accuracy = np.std(accuracies)\n",
    "        stat_group_results[probe_type]['mean'][group_name] = mean_accuracy\n",
    "        stat_group_results[probe_type]['std'][group_name] = std_accuracy\n",
    "\n",
    "# Map probe types to classifier names\n",
    "probe_type_to_name = {\n",
    "    TTPD: 'TTPD',\n",
    "    LRProbe: 'LR',\n",
    "    CCSProbe: 'CCS',\n",
    "    MMProbe: 'MM'\n",
    "}\n",
    "\n",
    "# Create DataFrames for mean accuracies and standard deviations\n",
    "group_names = ['Conjunctions', 'Disjunctions', 'Affirmative German', 'Negated German', 'common_claim_true_false', 'counterfact_true_false']\n",
    "classifier_names = ['TTPD', 'LR', 'CCS', 'MM']\n",
    "\n",
    "mean_df = pd.DataFrame(index=group_names, columns=classifier_names)\n",
    "std_df = pd.DataFrame(index=group_names, columns=classifier_names)\n",
    "\n",
    "for probe_type in probe_types:\n",
    "    classifier_name = probe_type_to_name[probe_type]\n",
    "    for group_name in group_names:\n",
    "        mean_accuracy = stat_group_results[probe_type]['mean'][group_name]\n",
    "        std_accuracy = stat_group_results[probe_type]['std'][group_name]\n",
    "        mean_df.loc[group_name, classifier_name] = mean_accuracy\n",
    "        std_df.loc[group_name, classifier_name] = std_accuracy\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(6.5, 6), ncols=4)\n",
    "\n",
    "for idx, classifier_name in enumerate(classifier_names):\n",
    "    ax = axes[idx]\n",
    "    mean_values = mean_df[classifier_name].values.astype(float)\n",
    "    std_values = std_df[classifier_name].values.astype(float)\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(mean_values[:, np.newaxis], vmin=0, vmax=1, cmap='plasma', aspect='auto')\n",
    "    \n",
    "    # Annotate the heatmap with mean accuracies and standard deviations\n",
    "    for i in range(len(group_names)):\n",
    "        mean_accuracy = mean_values[i]\n",
    "        std_accuracy = std_values[i]\n",
    "        ax.text(0, i, f'{round(mean_accuracy * 100):2d} $\\pm$ {round(std_accuracy * 100):2d}', ha='center', va='center', fontsize=14)\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    if idx == 0:\n",
    "        ax.set_yticks(np.arange(len(group_names)))\n",
    "        ax.set_yticklabels(group_names, fontsize=15)\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "    ax.set_title(classifier_name, fontsize=15)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "cbar.ax.tick_params(labelsize=13)\n",
    "\n",
    "fig.suptitle(\"Classification Accuracies\", fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real world scenarios / lies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "probe_types = [TTPD, LRProbe, CCSProbe, MMProbe]\n",
    "results = {TTPD: [], LRProbe: [], CCSProbe: [], MMProbe: []}\n",
    "num_iter = 50\n",
    "\n",
    "total_iterations = len(probe_types) * num_iter\n",
    "with tqdm(total=total_iterations, desc=\"Training and evaluating classifiers\") as pbar: # progress bar\n",
    "    for probe_type in probe_types:\n",
    "        for n in range(num_iter):\n",
    "            # load training data\n",
    "            acts_centered, acts, labels, polarities = collect_training_data(train_sets, train_set_sizes, model_family,\n",
    "                                                                                           model_size, model_type,layer)\n",
    "            if probe_type == TTPD:\n",
    "                probe = TTPD.from_data(acts_centered, acts, labels, polarities)\n",
    "            if probe_type == LRProbe:\n",
    "                probe = LRProbe.from_data(acts, labels)\n",
    "            if probe_type == CCSProbe:\n",
    "                acts_affirm = acts[polarities == 1.0]\n",
    "                acts_neg = acts[polarities == -1.0]\n",
    "                labels_affirm = labels[polarities == 1.0]\n",
    "                mean_affirm = torch.mean(acts_affirm, dim=0) \n",
    "                mean_neg = torch.mean(acts_neg, dim=0)\n",
    "                acts_affirm = acts_affirm - mean_affirm\n",
    "                acts_neg = acts_neg - mean_neg\n",
    "                probe = CCSProbe.from_data(acts_affirm, acts_neg, labels_affirm, device=device).to('cpu')\n",
    "            if probe_type == MMProbe:\n",
    "                probe = MMProbe.from_data(acts, labels)\n",
    "\n",
    "            # evaluate classification accuracy on real world scenarios\n",
    "            dm = DataManager()\n",
    "            real_world_dataset = \"real_world_scenarios/all_unambiguous_replies\"\n",
    "            dm.add_dataset(real_world_dataset, model_family, model_size, model_type, layer, split=None, center=False, device='cpu')\n",
    "            acts, labels = dm.data[real_world_dataset]\n",
    "            \n",
    "            # classifier specific predictions\n",
    "            if probe_type == CCSProbe:\n",
    "                acts = acts - (mean_affirm + mean_neg)/2\n",
    "\n",
    "            predictions = probe.pred(acts)\n",
    "            results[probe_type].append((predictions == labels).float().mean().item())\n",
    "            pbar.update(1)\n",
    "\n",
    "for probe_type in probe_types:\n",
    "    mean = np.mean(results[probe_type])\n",
    "    std = np.std(results[probe_type])\n",
    "    print(f\"{probe_type.__name__}:\")\n",
    "    print(f\"  Mean Accuracy: {mean*100:.2f}%\")\n",
    "    print(f\"  Standard Deviation: {std*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
